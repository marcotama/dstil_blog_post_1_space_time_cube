:last-update-label!:
:bibtex-file: bibliography.bib
:xrefstyle: short

= Visualising Space-Time Data
TODO: introduction to what the blog post is about

== The problem
Visually analysing time-dependent data is a common task in many fields. This however becomes problematic when the data is collected in a spatial context. The longstanding problem is: how does one visualise 2- or 3-dimensional data that evolves over time using computer screens and paper? In other words, how do you visualise 4-dimensional data on mediums that are inherently 2-dimensional?
There are different approaches one can take.

The first option is *reducing the dimensionality* of your data. For example, link:https://en.wikipedia.org/wiki/Principal_component_analysis[Principal Component Analysis] and link:https://en.wikipedia.org/wiki/Self-organizing_map[Self-Organising Maps] can sometimes be used to produce a "good-enough" 2-dimensional representation of 3-dimensional data. More simply, some problems do not require all of the original dimensions: for example, movement data on a flat terrain do not require an extra dimension to represent altitude.

A second option is to *use time as an extra dimension*. This is nothing new: for more than a century people have been using a fast-changing sequence of images to trick the eye in perceiving something as smoothly moving over time. We call those movies, animations, films and so on. Animations augment your medium of visualisation by a dimension, giving you more space of maneuver to represent your data.

Another option is to *use stereoscopic images* to transmit to the brain a third dimension using a 2-dimensional medium. The method relies on the ability of the brain to "understand" two images (of a 3-dimensional scene) taken at slightly different angles as 3-dimensional scene. This, however, requires that each image is sent to one of the eyes; options to accomplish this include 1) link:https://en.wikipedia.org/wiki/Virtual_reality_headset[Virtual Reality headsets], 2) link:https://en.wikipedia.org/wiki/Polarized_3D_system[polarised glasses], 3) link:https://en.wikipedia.org/wiki/Anaglyph_3D[bi-colored goggles] and 4) simply crossing your eyes (see <<stereogram>>). The requirement for additional equipment and the awkwardness of crossing one's eyes makes this option not widely viable.
[#stereogram]
.Hyades - the movement of stars in 300,000 years. To view this picture you need link:https://en.wikipedia.org/wiki/Stereoscopy#Side-by-side[cross-eyed viewing] (source: link:https://cs.wikipedia.org/wiki/Soubor:Astro_4D_hyades_cr_anim.gif[Wikipedia]).
[link=https://cs.wikipedia.org/wiki/Soubor:Astro_4D_hyades_cr_anim.gif]
image::images/4D_hyades.gif[Hyades in 3D through time,300,200,align="center"]

One more option is to *simulate an extra dimension* on a 2-dimensional medium. This has been done for decades in movies and video-games: the scene is shown as flat, but the brain perceives it in 3 dimensions. This can be accomplished using link:https://en.wikipedia.org/wiki/Perspective_(graphical)[perspective] and link:https://en.wikipedia.org/wiki/Parallax[parallax movement], which transmit depth information to the brain.

In this blog post we focus on the latter option, and in particular on how to achieve an interactive 3-dimensional visualisation in Python using the link:https://plot.ly/[Plotly] library. Plotly provides the perspective representation out-of-the box, and its plots are interactive, which provides the movement much needed by the brain.
This type of visualisation is called a _space-time cube_, and was invented in the 70s by Torsten HÃ¤gerstraand cite:[hagerstraand1970people].


== Plotly limitation and workaround
When representing data whose context is spatial, plotting a terrain is most helpful to understand "where" the data points are located. Unfortunately, Plotly at the moment link:https://github.com/plotly/plotly.js/issues/1650[does not support] plotting a 2-dimensional image in a 3-dimensional plot (e.g. sticking it onto a plan embedded in the 3D scene).

However, it does support coloring a flat surface with a _colormap_ based on some value. We will bend this feature to our needs; that is, to visually represent an approximation of a terrain. The trick we will use involves two steps:

 . associating a scalar value to each pixel, and
 . associating values to colors, the same way link:https://commons.wikimedia.org/wiki/File:Heatmap_birthday_rank_USA.svg[heatmaps] do.

The trick is to create a custom colormap. Our colormap, instead of going from, say, green to black, will include a selection of colors representative enough that we can draw the map using those colors only.


As a use-case, we will take a video game, link:http://www.dota2.com/[_Defense of the Ancients 2_] (more commonly known as _DotA 2_). The game sees 2 teams of 5 players each, each team controlling half of the map, and having a base deep in their controlled area. The two teams fight each other with the objective to destroy the opponents base. <<dota2_map>> shows a sketch of the map.

[#dota2_map]
.The map used in DotA 2 (version 7.07, source: https://dota2.gamepedia.com/File:Minimap_7.07.png[Gamepedia]).
[link=https://dota2.gamepedia.com/File:Minimap_7.07.png]
image::images/dota2_minimap.png[Hyades in 3D through time,300,200,align="center"]

To start with, we need to reduce the number of colors present in the image. We can achieve this in several ways. To keep this blog post limited, we will use an external tool, link:http://www.imagemagick.org/Usage/quantize/#colors[ImageMagick] to do the job. While we are at it, we will also crop and resize the image, because high resolutions are rendered slowly and high numbers of colours will look worse (more on this later).

.Resize and reduce the number of colors in the image
[#listing:resize-and-dither-image,bash]
----
convert Minimap_7.07.png \
  -crop 1000x940+10+30 \
  -resize 160000@ \
  +dither -colors 32 \
  Minimap_7.07_adjusted.png
----


We can load and display the image using Python with the following code.

.Load and show map
[#listing:load-image,python]
----
img = imread('Minimap_7.07_adjusted.png')
imshow(img)
print(img.dtype, img.shape)
----

Python informs us that this is image has a resolution of 387x412, with 4 values for the colors (RGB plus alpha).
The next step is to find the unique colors chosen by ImageMagick; that is, all the colors present in the image.
We can also display the colors found.

.Find colors palette
[#listing:find-colors,python]
----
# Find colors
img_array = img[:, :, :3].reshape((img.shape[0] * img.shape[1], 3))
colors = np.unique(img_array, axis=0)
n_colors = colors.shape[0]
print(colors.shape)

# Show colors
def show_colors(colors):
    colors_matrix = np.reshape(colors, [4, n_colors // 4, 3])
    imshow(np.reshape(colors, (1, -1, 3)))
    plt.xticks([])
    plt.yticks([])
    plt.gcf().set_size_inches(10, 1)
show_colors(colors)
----

Python informs us that there are 32 colors, which is what we expect since that is how many colours we asked ImageMagick to use.
We then use the colors to create a custom Plotly colormap, which is nothing but a list of tuples with a float as first element and a string in the format `'rgb({}, {}, {})'` as second element, with integer values in the range 0-255. Plotly uses link:https://en.wikipedia.org/wiki/Linear_interpolation[linear interpolation] to determine the colors in between the points we provide.

.Create a custom Plotly colormap
[#listing:create-colormap,python]
----
color_to_value = {tuple(color[:3]): i / (n_colors - 1) for i, color in enumerate(colors)}
my_cmap_ply = [(value, 'rgb({}, {}, {})'.format(*color)) for color, value in color_to_value.items()]
----

Now that we have a colormap, we map each pixel to the value that in the colormap corresponds to its color.

.Convert RGBs to appropriate values based on the colormap
[#listing:convert-pixels,python]
----
fun_find_value = lambda x: color_to_value[tuple(x[:3])]
values = np.apply_along_axis(fun_find_value, 2, img)
----

We are now ready to show (and save) the terrain map in a 3-dimensional plot.

.Show terrain
[#listing:show-terrain,python]
----
yy = np.linspace(0, 1, img.shape[0])
xx = np.linspace(0, 1, img.shape[1])
zz = np.zeros(img.shape[:2])

surf = go.Surface(
    x=xx, y=yy, z=zz,
    colorscale=my_cmap_ply,
    surfacecolor=values,
    showscale=False
)
fig = go.Figure(data=go.Data([surf]), layout=go.Layout())


plot(fig, filename='dota2-terrain.html', auto_open=False)
iplot(fig)
----


TODO: add lines for data


=== Bonus
If you zoom in on the terrain, you can notice artifacts between colors. This is an unwanted consequence of bringing down a 3-dimensional space (the space of RGB colors) to a 1-dimensional space (the colormap). When we did that, we artificially put an order to the colors, and now when Plotly wants to smoothly transition from color A to color B, it will use all the colors in between A and B.
Since the colors are arranged in whatever order numpy found them in, it can happen that black is in between two shades of green: this will cause there being a black line between any two adjacent pixels with those shades of green. There is no way around this, but there is a way to mitigate this effect: sorting the colors such that they are in a "visually smooth" order, whatever that means.

The way I approached this problem is to find the "shortest visual path" through all the colors. This is an instance of the famous link:https://en.wikipedia.org/wiki/Travelling_salesman_problem[Traveling Salesman Problem], and as such is NP-hard. As far as science knows, there is no way to  solve this problem efficiently; that is, there is no way to find _the best_ solution.
However, there are a number of algorithms to compute approximate solutions. The first coming to my mind are link:https://en.wikipedia.org/wiki/Metaheuristic[meta-heuristics], strategies to solve optimisation problems that "tend to work"; the most famous examples are evolutionary computation algorithms. Conveniently, there is a Python package that approximately solves the TSP problem using link:https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms[ant-colony optimisation], link:https://pypi.org/project/ACO-Pants/[ACO-pants].

We can use the library to solve our colors sorting problem if we can provide a function computing our notion of "visual distance" between two colors. It turns out this is a common enough problem that standards have been created and that a Python package exists implementing them. Enter link:https://en.wikipedia.org/wiki/Color_difference#CIEDE2000[Delta E CIE 2000] and the link:https://python-colormath.readthedocs.io/en/latest/delta_e.html#colormath.color_diff.delta_e_cie2000[colormath] package.

We are all set to solve the problem.

.Sort colors "visually"
[#listing:sort-colors,python]
----
# Sort colors
from colormath.color_diff import delta_e_cie2000
from colormath.color_objects import LabColor, sRGBColor
from colormath.color_conversions import convert_color
from pants import World, Solver
def rgb_distance(color1, color2):
    color1 = sRGBColor(*color1)
    color2 = sRGBColor(*color2)
    color1 = convert_color(color1, LabColor)
    color2 = convert_color(color2, LabColor)
    return float(delta_e_cie1976(color1, color2))
colors = [tuple(c) for c in colors]
solution = Solver().solve(World(colors, rgb_distance))
colors = np.array(solution.tour)
print(colors.shape)
show_colors(colors)
----

== Bibliography
bibliography::[]